{
  "name": "Meta Llama 7B",
  "transformerType": "META_LLAMA",
  "repo": "https://huggingface.co/huggyllama/llama-7b",
  "files": [
    "config.json",
    "model-00001-of-00002.safetensors",
    "model-00002-of-00002.safetensors"
  ],
  "parameterNaming": "model.{name}",
  "decoderParameterNaming": "model.layers.{decoderId}.{name}",
  "memorySize": 30720,
  "quantize": {
    "quantizationType": "QLoRA",
    "outputFloatType": "FLOAT_16",
    "config": {
      "variant": "fp4",
      "blockSize": 128
    }
  }
}
